import argparse
import logging
import multiprocessing
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path

import ChatTTS
import torch
import torchaudio
from pydub import AudioSegment
from pydub.silence import detect_leading_silence

from .pronunciations import DEFAULT_PRONUNCIATIONS


def setup_logging(log_file=None):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    if log_file is None:
        # é»˜è®¤æ—¥å¿—æ–‡ä»¶ï¼šlogs/generate_podcast_YYYYMMDD.log
        # ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ logs ç›®å½•
        log_dir = Path.cwd() / "logs"
        log_dir.mkdir(exist_ok=True)
        log_file = log_dir / f"generate_podcast_{datetime.now().strftime('%Y%m%d')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


def count_text_chars(text):
    """ç»Ÿè®¡å®é™…æ–‡å­—æ•°é‡ï¼ˆå»é™¤æ ‡è®°å’Œæ§åˆ¶å­—ç¬¦ï¼‰"""
    # ç§»é™¤ ChatTTS æ ‡è®°ï¼š[uv_break], [laugh], [oral] ç­‰
    text_clean = re.sub(r'\[.*?\]', '', text)
    # ç§»é™¤ç©ºç™½å­—ç¬¦
    text_clean = re.sub(r'\s+', '', text_clean)
    return len(text_clean)


def read_markdown_file(file_path):
    """è¯»å– markdown æ–‡ä»¶å¹¶æå–æ–‡æœ¬å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {file_path}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}")
        sys.exit(1)


# å…¨å±€ ChatTTS å®ä¾‹ï¼ˆé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼‰
_chat_instance = None


def initialize_worker():
    """å¤šè¿›ç¨‹ Worker åˆå§‹åŒ–ï¼šæ¯ä¸ªè¿›ç¨‹åŠ è½½ä¸€æ¬¡æ¨¡å‹"""
    global _chat_instance
    if _chat_instance is None:
        _chat_instance = get_chat_instance()


def get_chat_instance():
    """è·å– ChatTTS å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _chat_instance
    if _chat_instance is not None:
        return _chat_instance

    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
    # ä¼˜å…ˆæŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„ asset
    project_root = Path.cwd()
    asset_dir = project_root / "asset"
    local_model_exists = (
        (asset_dir / "Decoder.safetensors").exists() and
        (asset_dir / "DVAE.safetensors").exists() and
        (asset_dir / "Embed.safetensors").exists() and
        (asset_dir / "Vocos.safetensors").exists() and
        (asset_dir / "gpt" / "config.json").exists() and
        (asset_dir / "gpt" / "model.safetensors").exists() and
        (asset_dir / "tokenizer" / "tokenizer.json").exists()
    )
    
    chat = ChatTTS.Chat()
    
    if local_model_exists:
        print(f"[Process {os.getpid()}] ğŸ”„ æ­£åœ¨åŠ è½½ ChatTTS æ¨¡å‹ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼‰...")
        # ChatTTS ä¼šè‡ªåŠ¨æ£€æµ‹å½“å‰ç›®å½•ä¸‹çš„ asset æ–‡ä»¶å¤¹
        # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° asset ç›®å½•
        original_cwd = os.getcwd()
        try:
            os.chdir(str(project_root))
            chat.load(compile=False)  # compile=False å¯ä»¥åŠ å¿«åŠ è½½é€Ÿåº¦
            print(f"[Process {os.getpid()}] âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼‰")
        finally:
            os.chdir(original_cwd)
    else:
        print(f"[Process {os.getpid()}] ğŸ”„ æ­£åœ¨åŠ è½½ ChatTTS æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä»ç½‘ç»œä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼‰...")
        print("ğŸ’¡ æç¤ºï¼šè¿è¡Œ download_models.sh å¯ä»¥é¢„å…ˆä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼ŒåŠ å¿«åç»­åŠ è½½é€Ÿåº¦")
        chat.load(compile=False)  # compile=False å¯ä»¥åŠ å¿«åŠ è½½é€Ÿåº¦
        print(f"[Process {os.getpid()}] âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
    _chat_instance = chat
    return _chat_instance


def apply_pronunciations(text, dictionary):
    """Apply pronunciation replacements from dictionary (case-insensitive for keys)"""
    if not dictionary:
        return text
        
    for word, replacement in dictionary.items():
        # Use simple string replacement for now, or regex for whole words
        # Replaces all occurrences, case-insensitive logic handled by user input typically
        # But here we do simple replace to keep it predictable
        text = text.replace(str(word), str(replacement))
    return text


def match_target_amplitude(sound, target_dBFS):
    """Standardize loudness to target dBFS"""
    change_in_dBFS = target_dBFS - sound.dBFS
    return sound.apply_gain(change_in_dBFS)


def generate_audio(text, voice, output_file, rate=None, pitch=None, logger=None, pronunciations=None):
    """ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨ ChatTTSï¼‰"""
    if logger is None:
        logger = logging.getLogger(__name__)
    
    # Check if text is empty
    if not text or not text.strip():
        logger.warning(f"Empty text for output {output_file}, skipping generation.")
        return

    # Apply pronunciation replacements
    # Merge user provided pronunciations with defaults
    # User config overrides defaults
    combined_pronunciations = DEFAULT_PRONUNCIATIONS.copy()
    if pronunciations:
        combined_pronunciations.update(pronunciations)

    # Use combined dictionary
    if combined_pronunciations:
        original_text = text
        text = apply_pronunciations(text, combined_pronunciations)
        if text != original_text:
            logger.info(f"  Applied pronunciation fixes. Text modified.")

    chat = get_chat_instance()
    
    # ç»Ÿè®¡æ–‡å­—æ•°é‡
    text_chars = count_text_chars(text)
    raw_chars = len(text)
    
    # å°† voice å‚æ•°è½¬æ¢ä¸º seed
    try:
        seed = int(voice) if voice.isdigit() else 2222
    except (ValueError, AttributeError):
        seed = 2222
    
    # ChatTTS ä¸æ”¯æŒ rate å’Œ pitch å‚æ•°ï¼Œç»™å‡ºæç¤º
    if rate or pitch:
        logger.warning("ChatTTS ä¸æ”¯æŒè¯­é€Ÿå’ŒéŸ³è°ƒè°ƒæ•´ï¼Œè¿™äº›å‚æ•°å°†è¢«å¿½ç•¥")
    
    # è®¾ç½®å„åº“éšæœºç§å­ï¼Œç¡®ä¿æè‡´ç¨³å®šæ€§
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # ç”Ÿæˆç¨³å®šçš„ speaker embedding
    # [Fix] ç§»é™¤é‡å¤è°ƒç”¨ï¼Œç¡®ä¿é€»è¾‘å”¯ä¸€
    spk_emb = chat.sample_random_speaker()
    
    logger.info(f"å¼€å§‹ç”ŸæˆéŸ³é¢‘ - seed: {seed}, åŸå§‹æ–‡æœ¬é•¿åº¦: {raw_chars} å­—ç¬¦, å®é™…æ–‡å­—æ•°: {text_chars} å­—")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # --- é˜¶æ®µ 1: æ–‡æœ¬å½’ä¸€åŒ– (Source of Truth) ---
    # ChatTTS normalizer natively preserves [break_n] and other [tag] formats.
    normalized_text = chat.normalizer(text, do_text_normalization=True, do_homophone_replacement=True)
    logger.info(f"  normalized_text: {repr(normalized_text)}")
    logger.info("  1. æ–‡æœ¬å½’ä¸€åŒ–å®Œæˆ")

    # --- é˜¶æ®µ 2: æ–‡æœ¬æ¶¦è‰² (Source of Prosody) ---
    logger.info("  2. æ­£åœ¨è¿›è¡Œæ–‡æœ¬æ¶¦è‰² (è·å–è¯­æ°”Tags)...")
    
    # [Optimize] å½»åº•å‰¥ç¦»æ‰€æœ‰ç¬¦å·å’Œæ¢è¡Œï¼Œä»…ä¿ç•™çº¯æ–‡å­—ä¾›æ¨¡å‹æ¶¦è‰²ã€‚
    # è¿™æ ·å¯ä»¥é˜²æ­¢ [break_6] ç­‰æ ‡ç­¾å¹²æ‰°æ¨¡å‹å¯¼è‡´å…¶è¿›å…¥å¹»å¬å¾ªç¯ã€‚
    # ç”±äºåç»­æœ‰ align_text é€»è¾‘ï¼Œæ ‡ç­¾ä¼šåœ¨æ¨ç†å‰è¢«è‡ªåŠ¨æ‰¾å›ã€‚
    text_for_model = re.sub(r'\[.*?\]', '', text) # ç§»é™¤æ‰€æœ‰ [tag]
    text_for_model = re.sub(r'\s+', '', text_for_model) # ç§»é™¤æ¢è¡Œå’Œç©ºæ ¼
    
    # è‡ªç„¶åº¦ä¼˜å…ˆï¼šRefine 0.7 æä¾›æ›´ä¸°å¯Œçš„è¯­æ°”èµ·ä¼
    params_refine = chat.RefineTextParams(
        temperature=0.7,
        top_P=0.7,
        prompt='[laugh_0][break_4]', 
        max_new_token=1024,
        manual_seed=seed
    )
    
    refined_text_raw = chat.infer(
        [text_for_model],
        params_refine_text=params_refine,
        refine_text_only=True,
        split_text=True
    )
    
    # Handle inference result (list or string)
    if isinstance(refined_text_raw, list):
        refined_text_combined = " ".join(refined_text_raw)
    else:
        refined_text_combined = refined_text_raw

    # --- é˜¶æ®µ 3: æ–‡æœ¬å¯¹é½ (Alignment) ---
    logger.info("  3. æ­£åœ¨æ‰§è¡Œæ–‡æœ¬å¯¹é½ (å»é™¤å¹»è§‰)...")
    from .text_aligner import align_text
    aligned_text = align_text(normalized_text, refined_text_combined)
    
    # ç»Ÿè®¡ä¿®æ­£æƒ…å†µ
    if aligned_text != refined_text_combined:
        diff_len = len(refined_text_combined) - len(aligned_text)
        logger.info(f"     âœ… å¯¹é½ä¿®æ­£å®Œæˆ (å·®å¼‚å­—ç¬¦æ•°: {diff_len})")
    
    # --- é˜¶æ®µ 4: éŸ³é¢‘æ¨ç† (Infer) ---
    logger.info("  4. æ­£åœ¨ç”ŸæˆéŸ³é¢‘æ³¢å½¢...")
    
    # æ¨ç†æ¸©åº¦ä¿æŒ 0.3 ä»¥é”å®šéŸ³è‰²ï¼Œå»é™¤è¯­é€Ÿ prompt å¢åŠ è‡ªç„¶åº¦
    params_infer = chat.InferCodeParams(
        spk_emb=spk_emb, 
        max_new_token=2048,
        temperature=0.3, 
        top_P=0.7,
        prompt='', # å»é™¤å›ºå®šè¯­é€Ÿï¼Œè®©æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡è‡ªç„¶å‘æŒ¥
        manual_seed=seed
    )
    
    # [Safety] Final scrub: Ensure only standard tags exist in the final string
    final_text = re.sub(r'\[\s*uv_break\s*\]', '[break_6]', aligned_text, flags=re.IGNORECASE)
    whitelisted_prefixes = ['break_', 'laugh', 'oral_', 'speed_']
    
    def tag_safety_filter(match):
        tag = match.group(0)
        inner = tag[1:-1].lower()
        if any(inner.startswith(p) for p in whitelisted_prefixes):
            return tag
        logger.warning(f"     ğŸ›¡ï¸  Safety Filter: Dropping suspicious tag {tag}")
        return ""
        
    final_text = re.sub(r'\[.*?\]', tag_safety_filter, final_text)
    
    # [Debug] Log the definitive text string
    logger.info(f"  Final Inference Text: {repr(final_text)}")
    
    # [Optimize] Disable split_text for segments shorter than 200 chars to prevent voice drift between splits
    # Also ensure no weird whitespace is triggering internal splitting
    final_text = re.sub(r'\s+', ' ', final_text).strip()
    
    wavs = chat.infer(
        [final_text], 
        use_decoder=True, 
        params_infer_code=params_infer,
        skip_refine_text=True, # Critical: Don't refine again!
        do_text_normalization=False, # It's already been normalized/refined
        split_text=True # Restore splitting for natural rhythm in longer segments
    )
    
    # è®°å½•ç”Ÿæˆæ—¶é—´
    generation_time = time.time() - start_time
    
    # è½¬æ¢ä¸º torch tensor
    wav_array = wavs[0]
    wav_tensor = torch.from_numpy(wav_array)
    
    # è®¡ç®—éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
    audio_duration = len(wav_array) / 24000  # é‡‡æ ·ç‡ 24000
    
    # ç¡®ä¿æ˜¯ 2D tensor (channels, samples)
    if len(wav_tensor.shape) == 1:
        wav_tensor = wav_tensor.unsqueeze(0)
    elif len(wav_tensor.shape) > 2:
        wav_tensor = wav_tensor[0] if wav_tensor.shape[0] == 1 else wav_tensor.squeeze()
    
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶æ‰©å±•åä¸º .wav
    output_path = Path(output_file)
    if output_path.suffix != '.wav':
        output_file = str(output_path.with_suffix('.wav'))
    
    # ä¿å­˜éŸ³é¢‘ï¼ˆé‡‡æ ·ç‡ 24000ï¼‰
    save_start_time = time.time()
    if wav_tensor.shape[0] > wav_tensor.shape[-1]:
        wav_tensor = wav_tensor.T
    torchaudio.save(output_file, wav_tensor, 24000)
    
    # --- é˜¶æ®µ 5: éŸ³é¢‘åå¤„ç† (Post-Processing) ---
    # 1. è‡ªåŠ¨åˆ‡é™¤å‰åé™éŸ³
    # 2. å“åº¦æ ‡å‡†åŒ– (-20 dBFS)
    try:
        sound = AudioSegment.from_wav(output_file)
        
        # åˆ‡é™¤é™éŸ³ (é˜ˆå€¼ -50dB)
        start_trim = detect_leading_silence(sound, -50.0)
        end_trim = detect_leading_silence(sound.reverse(), -50.0)
        # ç»™å¼€å¤´ç•™ 30ms ç¼“å†²ï¼Œé¿å…åˆ‡å¾—å¤ªæ­»
        start_trim = max(0, start_trim - 30)
        end_trim = max(0, end_trim - 30)
        sound = sound[start_trim:len(sound)-end_trim]
        
        # å“åº¦åŒ¹é…
        normalized_sound = match_target_amplitude(sound, -20.0)
        normalized_sound.export(output_file, format="wav")
        logger.info(f"  5. éŸ³é¢‘åå¤„ç†å®Œæˆ (åˆ‡é™¤é™éŸ³ + -20.0 dBFS)")
    except Exception as e:
        logger.error(f"  âŒ å“åº¦æ ‡å‡†åŒ–å¤±è´¥: {e}")

    save_time = time.time() - save_start_time
    
    total_time = time.time() - start_time
    
    # è®¡ç®—é€Ÿåº¦æŒ‡æ ‡
    chars_per_second = text_chars / generation_time if generation_time > 0 else 0
    audio_ratio = audio_duration / generation_time if generation_time > 0 else 0
    
    # è®°å½•ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"éŸ³é¢‘ç”Ÿæˆå®Œæˆ - æ–‡ä»¶: {output_file}")
    logger.info(f"  ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"    - åŸå§‹æ–‡æœ¬é•¿åº¦: {raw_chars} å­—ç¬¦")
    logger.info(f"    - å®é™…æ–‡å­—æ•°: {text_chars} å­—")
    logger.info(f"    - ç”Ÿæˆè€—æ—¶: {generation_time:.2f} ç§’")
    logger.info(f"    - ä¿å­˜è€—æ—¶: {save_time:.2f} ç§’")
    logger.info(f"    - æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"    - éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f} ç§’")
    logger.info(f"    - ç”Ÿæˆé€Ÿåº¦: {chars_per_second:.2f} å­—/ç§’")
    logger.info(f"    - éŸ³é¢‘/ç”Ÿæˆæ¯”: {audio_ratio:.2f}x")
    
    print(f"âœ… ç”Ÿæˆå®Œæ¯•: {output_file} ({generation_time:.2f}s, {audio_duration:.2f}s audio)")


def main():
    parser = argparse.ArgumentParser(
        description="ä» Markdown æ–‡ä»¶ç”Ÿæˆæ’­å®¢éŸ³é¢‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python generate_podcast.py script.md
  python generate_podcast.py script.md -o output.wav
  python generate_podcast.py script.md -v 2222
  python generate_podcast.py script.md -v 3333
        """
    )
    
    parser.add_argument(
        'input_file',
        type=str,
        help='è¾“å…¥çš„ Markdown æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='è¾“å‡ºéŸ³é¢‘æ–‡ä»¶åï¼ˆé»˜è®¤ï¼šè¾“å…¥æ–‡ä»¶å.mp3ï¼‰'
    )
    
    parser.add_argument(
        '-v', '--voice',
        type=str,
        default='2222',
        help='éšæœºç§å­ï¼ˆseedï¼‰ï¼Œç”¨äºæ§åˆ¶éŸ³è‰²ï¼Œé»˜è®¤ï¼š2222'
    )
    
    parser.add_argument(
        '-r', '--rate',
        type=str,
        default=None,
        help='è¯­é€Ÿè°ƒæ•´ï¼ˆChatTTS ä¸æ”¯æŒï¼Œå°†è¢«å¿½ç•¥ï¼‰'
    )
    
    parser.add_argument(
        '-p', '--pitch',
        type=str,
        default=None,
        help='éŸ³è°ƒè°ƒæ•´ï¼ˆChatTTS ä¸æ”¯æŒï¼Œå°†è¢«å¿½ç•¥ï¼‰'
    )
    
    parser.add_argument(
        '--log-file',
        type=str,
        default=None,
        help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šlogs/generate_podcast_YYYYMMDD.logï¼‰'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.log_file)
    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {args.input_file}")
    
    # è¯»å– markdown æ–‡ä»¶
    text = read_markdown_file(args.input_file)
    
    if not text:
        logger.error("æ–‡ä»¶å†…å®¹ä¸ºç©º")
        print("âŒ è­¦å‘Šï¼šæ–‡ä»¶å†…å®¹ä¸ºç©º")
        sys.exit(1)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤è¾“å‡ºåˆ° output/ ç›®å½•ï¼‰
    if args.output:
        output_file = args.output
    else:
        input_path = Path(args.input_file)
        # ç¡®ä¿ output ç›®å½•å­˜åœ¨
        output_dir = Path("output")
        output_dir.mkdir(exist_ok=True)
        output_file = str(output_dir / (input_path.stem + ".mp3"))
    
    text_chars = count_text_chars(text)
    
    print(f"ğŸ“ è¯»å–æ–‡ä»¶: {args.input_file}")
    print(f"ğŸ² éšæœºç§å­: {args.voice}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦ (å®é™…æ–‡å­—: {text_chars} å­—)")
    print()
    
    logger.info(f"è¾“å…¥æ–‡ä»¶: {args.input_file}, è¾“å‡ºæ–‡ä»¶: {output_file}, seed: {args.voice}")
    logger.info(f"æ–‡æœ¬ç»Ÿè®¡: åŸå§‹é•¿åº¦ {len(text)} å­—ç¬¦, å®é™…æ–‡å­—æ•° {text_chars} å­—")
    
    # ç”ŸæˆéŸ³é¢‘
    generate_audio(text, args.voice, output_file, args.rate, args.pitch, logger)


if __name__ == "__main__":
    main()